{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c456910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 0.7000737190246582, Val Loss 0.6979249715805054\n",
      "Epoch 10: Train Loss 0.6954935193061829, Val Loss 0.6931849122047424\n",
      "Epoch 20: Train Loss 0.6928402781486511, Val Loss 0.6931735277175903\n",
      "Epoch 30: Train Loss 0.692310094833374, Val Loss 0.6934025883674622\n",
      "Epoch 40: Train Loss 0.6928223967552185, Val Loss 0.6935884952545166\n",
      "Epoch 50: Train Loss 0.6918793320655823, Val Loss 0.6937255263328552\n",
      "Epoch 60: Train Loss 0.6907819509506226, Val Loss 0.693825900554657\n",
      "Epoch 70: Train Loss 0.6910154819488525, Val Loss 0.6940131783485413\n",
      "Epoch 80: Train Loss 0.6909488439559937, Val Loss 0.6944582462310791\n",
      "Epoch 90: Train Loss 0.6897420287132263, Val Loss 0.6947230100631714\n",
      "\n",
      "Top 10 High-Risk Areas:\n",
      "     TC  Workgroup  ApplicationAge  PreviousRejectionRate  ComplexityScore  \\\n",
      "134   8          2       20.622919               0.423613        49.968232   \n",
      "114   7          1       21.935356               0.383025        48.527392   \n",
      "57    4          1       20.490379               0.445251        52.650364   \n",
      "135   8          3       20.406269               0.474604        47.363720   \n",
      "76    5          1       18.626740               0.513035        48.824298   \n",
      "38    3          1       20.254976               0.507980        48.854944   \n",
      "133   8          1       19.282781               0.530615        51.968522   \n",
      "115   7          2       20.149620               0.627308        49.961647   \n",
      "117   7          4       19.985420               0.462645        49.960795   \n",
      "97    6          3       19.442948               0.490704        50.500675   \n",
      "\n",
      "     PriorArtCitations  AllowanceRate  RiskLevel  RiskProbability  \n",
      "134          23.756757       0.522234   0.540541         0.533771  \n",
      "114          27.774194       0.434231   0.451613         0.530443  \n",
      "57           27.851852       0.546866   0.407407         0.529669  \n",
      "135          26.266667       0.514818   0.466667         0.529595  \n",
      "76           21.576923       0.524571   0.538462         0.529257  \n",
      "38           24.147059       0.481986   0.529412         0.529172  \n",
      "133          22.564103       0.506763   0.743590         0.528305  \n",
      "115          23.428571       0.415081   0.642857         0.526860  \n",
      "117          23.844444       0.519299   0.600000         0.525907  \n",
      "97           19.655172       0.547804   0.379310         0.524166  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class RiskIdentificationModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RiskIdentificationModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def generate_simulated_dataset():\n",
    "    # Simulated dataset with TC, workgroup, and risk features\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    data = {\n",
    "        'TC': np.random.randint(1, 9, n_samples),\n",
    "        #'Workgroup': np.random.randint(1, 20, n_samples),\n",
    "        # removing workgroup, this seems a little silly. \n",
    "        \n",
    "        'ApplicationAge': np.random.normal(20, 5, n_samples),\n",
    "        'PreviousRejectionRate': np.random.uniform(0, 1, n_samples),\n",
    "        'ComplexityScore': np.random.normal(50, 10, n_samples),\n",
    "        'PriorArtCitations': np.random.randint(0, 50, n_samples),\n",
    "        'AllowanceRate': np.random.uniform(0, 1, n_samples),\n",
    "        'RiskLevel': np.random.randint(0, 2, n_samples)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Prepare features and labels\n",
    "    features = ['TC', 'Workgroup', 'ApplicationAge', \n",
    "                'PreviousRejectionRate', 'ComplexityScore', \n",
    "                'PriorArtCitations', 'AllowanceRate']\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df['RiskLevel']\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(X_scaled)\n",
    "    y_tensor = torch.FloatTensor(y.values).unsqueeze(1)\n",
    "    \n",
    "    return X_tensor, y_tensor, scaler\n",
    "\n",
    "def train_risk_model(X_train, y_train, X_val, y_val):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = RiskIdentificationModel(input_dim)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss {loss.item()}, Val Loss {val_loss.item()}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def identify_high_risk_areas(model, X, df, scaler):\n",
    "    # Predict risk probabilities\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        risk_probs = model(X)\n",
    "    \n",
    "    # Add probabilities back to dataframe\n",
    "    df['RiskProbability'] = risk_probs.numpy()\n",
    "    \n",
    "    # Create a copy to work with for feature aggregation\n",
    "    df_with_prob = df.copy()\n",
    "    df_with_prob['RiskProbability'] = risk_probs.numpy()\n",
    "    \n",
    "    # Group by TC and Workgroup to get mean values of all features\n",
    "    high_risk_areas = df_with_prob.groupby(['TC', 'Workgroup']).agg({\n",
    "        'ApplicationAge': 'mean',\n",
    "        'PreviousRejectionRate': 'mean',\n",
    "        'ComplexityScore': 'mean',\n",
    "        'PriorArtCitations': 'mean',\n",
    "        'AllowanceRate': 'mean',\n",
    "        'RiskLevel': 'mean',\n",
    "        'RiskProbability': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by RiskProbability (descending)\n",
    "    high_risk_areas = high_risk_areas.sort_values('RiskProbability', ascending=False)\n",
    "    \n",
    "    return high_risk_areas\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Generate simulated dataset\n",
    "    df = generate_simulated_dataset()\n",
    "    \n",
    "    # Prepare data\n",
    "    X_tensor, y_tensor, scaler = prepare_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tensor, y_tensor, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = train_risk_model(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Identify high-risk areas\n",
    "    high_risk_areas = identify_high_risk_areas(model, X_tensor, df, scaler)\n",
    "\n",
    "       # Save high-risk areas to JSON file\n",
    "    output_file = \"high_risk_areas.json\"\n",
    "    \n",
    "    dir_path = os.path.dirname(output_file)\n",
    "    if dir_path == \"\":\n",
    "        dir_path = \".\"  # If no directory (i.e., file in current dir), just use current dir\n",
    "    else:\n",
    "        dir_path = dir_path  # Already has path\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Convert DataFrame to list of dicts (for JSON serialization)\n",
    "    high_risk_areas_json = high_risk_areas.to_dict(orient='records')\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(high_risk_areas_json, f, indent=4)\n",
    "    \n",
    "    print(\"\\nTop 10 High-Risk Areas:\")\n",
    "    print(high_risk_areas.head(10))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff4355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_application_age(application_data):\n",
    "    \"\"\"\n",
    "    Calculate application age from filing date to current date\n",
    "    \"\"\"\n",
    "    current_date = datetime.now()\n",
    "    application_age = (current_date - application_data['filing_date']).days / 365.25\n",
    "    return application_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b6289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rejection_rate(examiner_history):\n",
    "    \"\"\"\n",
    "    Calculate rejection rate based on historical office actions\n",
    "    \"\"\"\n",
    "    total_actions = len(examiner_history)\n",
    "    rejected_actions = sum(1 for action in examiner_history if action == 'rejected')\n",
    "    \n",
    "    rejection_rate = rejected_actions / total_actions if total_actions > 0 else 0\n",
    "    return rejection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_score(application_details):\n",
    "    \"\"\"\n",
    "    Multi-dimensional complexity scoring\n",
    "    \"\"\"\n",
    "    # Technical complexity factors\n",
    "    claim_complexity = len(application_details['claims'])\n",
    "    technical_domain_complexity = get_domain_complexity_factor(application_details['domain'])\n",
    "    \n",
    "    # Patent family size\n",
    "    patent_family_size = len(application_details['related_patents'])\n",
    "    \n",
    "    # Interdisciplinary score\n",
    "    interdisciplinary_score = calculate_interdisciplinary_score(application_details)\n",
    "    \n",
    "    # Weighted complexity calculation\n",
    "    complexity_score = (\n",
    "        0.3 * claim_complexity + \n",
    "        0.3 * technical_domain_complexity + \n",
    "        0.2 * patent_family_size + \n",
    "        0.2 * interdisciplinary_score\n",
    "    )\n",
    "    \n",
    "    return complexity_score\n",
    "\n",
    "def get_domain_complexity_factor(domain):\n",
    "    \"\"\"\n",
    "    Predefined complexity mapping for technical domains\n",
    "    \"\"\"\n",
    "    domain_complexity = {\n",
    "        'computer_science': 0.8,\n",
    "        'biotechnology': 0.9,\n",
    "        'mechanical_engineering': 0.6,\n",
    "        'electrical_engineering': 0.7\n",
    "        # Add more domains\n",
    "    }\n",
    "    return domain_complexity.get(domain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prior_art_citations(patent_application):\n",
    "    \"\"\"\n",
    "    Extract and analyze prior art citations\n",
    "    \"\"\"\n",
    "    # Patent database citation analysis\n",
    "    citations = patent_application['citations']\n",
    "    \n",
    "    # Citation metrics\n",
    "    citation_count = len(citations)\n",
    "    unique_citation_sources = len(set(citation['source'] for citation in citations))\n",
    "    \n",
    "    # Citation age analysis\n",
    "    current_year = datetime.now().year\n",
    "    citation_age_distribution = [\n",
    "        current_year - citation['year'] \n",
    "        for citation in citations\n",
    "    ]\n",
    "    \n",
    "    # Advanced citation analysis\n",
    "    citation_metrics = {\n",
    "        'total_citations': citation_count,\n",
    "        'unique_sources': unique_citation_sources,\n",
    "        'avg_citation_age': np.mean(citation_age_distribution),\n",
    "        'citation_diversity_score': calculate_citation_diversity(citations)\n",
    "    }\n",
    "    \n",
    "    return citation_metrics\n",
    "\n",
    "def calculate_citation_diversity(citations):\n",
    "    \"\"\"\n",
    "    Calculate diversity of citation sources\n",
    "    \"\"\"\n",
    "    source_domains = [citation['domain'] for citation in citations]\n",
    "    unique_domains = len(set(source_domains))\n",
    "    return unique_domains / len(citations) if citations else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6badab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_allowance_rate(examiner_history):\n",
    "    \"\"\"\n",
    "    Calculate patent allowance rate\n",
    "    \"\"\"\n",
    "    total_applications = len(examiner_history)\n",
    "    allowed_applications = sum(1 for app in examiner_history if app['status'] == 'allowed')\n",
    "    \n",
    "    allowance_rate = allowed_applications / total_applications if total_applications > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'allowance_rate': allowance_rate,\n",
    "        'total_applications': total_applications,\n",
    "        'allowed_applications': allowed_applications\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee64532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentDataWarehouse:\n",
    "    def __init__(self):\n",
    "        self.database = {\n",
    "            'applications': [],\n",
    "            'citations': [],\n",
    "            'examiner_history': []\n",
    "        }\n",
    "    \n",
    "    def collect_data(self, data_source):\n",
    "        # Implement data collection from various sources\n",
    "        pass\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Clean and standardize data\n",
    "        pass\n",
    "    \n",
    "    def feature_engineering(self):\n",
    "        # Generate complex features\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39219fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_level(application_data):\n",
    "    # Weighted risk factors\n",
    "    risk_components = {\n",
    "        'examination_complexity': 0.25,\n",
    "        'prior_rejection_history': 0.20,\n",
    "        'citation_complexity': 0.15,\n",
    "        'prosecution_history': 0.15,\n",
    "        'technical_domain_risk': 0.15,\n",
    "        'examiner_performance': 0.10\n",
    "    }\n",
    "    \n",
    "    # Detailed risk calculation\n",
    "    risk_scores = {\n",
    "        'examination_complexity': calculate_examination_complexity(application_data),\n",
    "        'prior_rejection_history': calculate_prior_rejection_risk(application_data),\n",
    "        'citation_complexity': calculate_citation_risk(application_data),\n",
    "        'prosecution_history': calculate_prosecution_history_risk(application_data),\n",
    "        'technical_domain_risk': calculate_technical_domain_risk(application_data),\n",
    "        'examiner_performance': calculate_examiner_performance_risk(application_data)\n",
    "    }\n",
    "    \n",
    "    # Weighted risk aggregation\n",
    "    total_risk_score = sum(\n",
    "        risk_scores[component] * weight \n",
    "        for component, weight in risk_components.items()\n",
    "    )\n",
    "    \n",
    "    # Risk level categorization\n",
    "    return categorize_risk_level(total_risk_score)\n",
    "\n",
    "def categorize_risk_level(risk_score):\n",
    "    \"\"\"\n",
    "    Convert continuous risk score to discrete risk levels\n",
    "    \"\"\"\n",
    "    if risk_score < 0.2:\n",
    "        return 0  # Low Risk\n",
    "    elif risk_score < 0.4:\n",
    "        return 1  # Medium-Low Risk\n",
    "    elif risk_score < 0.6:\n",
    "        return 2  # Medium Risk\n",
    "    elif risk_score < 0.8:\n",
    "        return 3  # High-Medium Risk\n",
    "    else:\n",
    "        return 4  # High Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_examination_complexity(application_data):\n",
    "    \"\"\"\n",
    "    Assess complexity of patent examination\n",
    "    \"\"\"\n",
    "    complexity_factors = [\n",
    "        len(application_data['claims']),\n",
    "        application_data['total_pages'],\n",
    "        application_data['unique_independent_claims']\n",
    "    ]\n",
    "    \n",
    "    # Normalize and aggregate complexity\n",
    "    normalized_complexity = [\n",
    "        (factor - min(complexity_factors)) / \n",
    "        (max(complexity_factors) - min(complexity_factors) + 1e-8)\n",
    "        for factor in complexity_factors\n",
    "    ]\n",
    "    \n",
    "    return np.mean(normalized_complexity)\n",
    "\n",
    "def calculate_prior_rejection_risk(application_data):\n",
    "    \"\"\"\n",
    "    Evaluate risk based on prior rejections\n",
    "    \"\"\"\n",
    "    rejection_history = application_data['rejection_history']\n",
    "    \n",
    "    # Multiple rejection indicators\n",
    "    risk_indicators = [\n",
    "        len(rejection_history),  # Number of rejections\n",
    "        any(rejection['type'] == 'final' for rejection in rejection_history),\n",
    "        sum(1 for rejection in rejection_history if rejection['type'] == 'non-final') > 2\n",
    "    ]\n",
    "    \n",
    "    # Weighted risk calculation\n",
    "    weights = [0.5, 0.3, 0.2]\n",
    "    return np.dot(risk_indicators, weights)\n",
    "\n",
    "def calculate_citation_risk(application_data):\n",
    "    \"\"\"\n",
    "    Assess risk through citation analysis\n",
    "    \"\"\"\n",
    "    citations = application_data['citations']\n",
    "    \n",
    "    citation_risk_factors = [\n",
    "        len(citations),  # Total citations\n",
    "        len(set(citation['source_country'] for citation in citations)),  # Citation diversity\n",
    "        sum(1 for citation in citations if citation['age'] < 5)  # Recent citations\n",
    "    ]\n",
    "    \n",
    "    # Normalize and aggregate\n",
    "    normalized_factors = [\n",
    "        (factor - min(citation_risk_factors)) / \n",
    "        (max(citation_risk_factors) - min(citation_risk_factors) + 1e-8)\n",
    "        for factor in citation_risk_factors\n",
    "    ]\n",
    "    \n",
    "    return np.mean(normalized_factors)\n",
    "\n",
    "def calculate_prosecution_history_risk(application_data):\n",
    "    \"\"\"\n",
    "    Evaluate prosecution complexity and duration\n",
    "    \"\"\"\n",
    "    prosecution_data = application_data['prosecution_history']\n",
    "    \n",
    "    risk_indicators = [\n",
    "        prosecution_data['total_office_actions'],\n",
    "        prosecution_data['prosecution_duration'],\n",
    "        prosecution_data['amendments_count']\n",
    "    ]\n",
    "    \n",
    "    # Normalize risk indicators\n",
    "    normalized_indicators = [\n",
    "        (indicator - min(risk_indicators)) / \n",
    "        (max(risk_indicators) - min(risk_indicators) + 1e-8)\n",
    "        for indicator in risk_indicators\n",
    "    ]\n",
    "    \n",
    "    return np.mean(normalized_indicators)\n",
    "\n",
    "def calculate_technical_domain_risk(application_data):\n",
    "    \"\"\"\n",
    "    Assess risk based on technical domain characteristics\n",
    "    \"\"\"\n",
    "    domain_risk_mapping = {\n",
    "        'biotechnology': 0.8,\n",
    "        'artificial_intelligence': 0.7,\n",
    "        'blockchain': 0.6,\n",
    "        'quantum_computing': 0.9,\n",
    "        'default': 0.5\n",
    "    }\n",
    "    \n",
    "    domain = application_data['technical_domain']\n",
    "    base_domain_risk = domain_risk_mapping.get(domain, domain_risk_mapping['default'])\n",
    "    \n",
    "    # Additional domain-specific risk factors\n",
    "    domain_complexity_factors = [\n",
    "        application_data['interdisciplinary_score'],\n",
    "        application_data['emerging_technology_indicator']\n",
    "    ]\n",
    "    \n",
    "    return base_domain_risk * np.mean(domain_complexity_factors)\n",
    "\n",
    "def calculate_examiner_performance_risk(application_data):\n",
    "    \"\"\"\n",
    "    Evaluate risk through examiner-specific factors\n",
    "    \"\"\"\n",
    "    examiner_data = application_data['examiner_profile']\n",
    "    \n",
    "    performance_indicators = [\n",
    "        examiner_data['allowance_rate'],\n",
    "        examiner_data['average_prosecution_time'],\n",
    "        examiner_data['technical_specialization_match']\n",
    "    ]\n",
    "    \n",
    "    # Inverse relationship with risk\n",
    "    risk_score = 1 - np.mean(performance_indicators)\n",
    "    \n",
    "    return risk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c484dc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    134\u001b[39m examiner_profile = {\n\u001b[32m    135\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mart_unit\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m2100\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    136\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtechnical_domains\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mcomputer_technology\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmachine_learning\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    137\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meducation\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mComputer Science\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    138\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresearch_publications\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mMachine Learning Techniques\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAI Applications\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    139\u001b[39m }\n\u001b[32m    141\u001b[39m patent_application = {\n\u001b[32m    142\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtechnical_domain\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33martificial_intelligence\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    143\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrelated_domains\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mmachine_learning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdata_science\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    144\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mclaims\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mA method for machine learning...\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    145\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mThe invention relates to artificial intelligence techniques...\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    146\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m specialization_match = \u001b[43mcalculate_technical_specialization_match\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexaminer_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatent_application\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTechnical Specialization Match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecialization_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcalculate_technical_specialization_match\u001b[39m\u001b[34m(examiner_profile, patent_application)\u001b[39m\n\u001b[32m      7\u001b[39m examiner_expertise = {\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprimary_art_unit\u001b[39m\u001b[33m'\u001b[39m: examiner_profile[\u001b[33m'\u001b[39m\u001b[33mart_unit\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtechnical_domains\u001b[39m\u001b[33m'\u001b[39m: examiner_profile[\u001b[33m'\u001b[39m\u001b[33mtechnical_domains\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meducation_background\u001b[39m\u001b[33m'\u001b[39m: examiner_profile[\u001b[33m'\u001b[39m\u001b[33meducation\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpublication_areas\u001b[39m\u001b[33m'\u001b[39m: examiner_profile[\u001b[33m'\u001b[39m\u001b[33mresearch_publications\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Patent application technical characteristics\u001b[39;00m\n\u001b[32m     15\u001b[39m application_tech_profile = {\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprimary_domain\u001b[39m\u001b[33m'\u001b[39m: patent_application[\u001b[33m'\u001b[39m\u001b[33mtechnical_domain\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msecondary_domains\u001b[39m\u001b[33m'\u001b[39m: patent_application[\u001b[33m'\u001b[39m\u001b[33mrelated_domains\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mclaim_keywords\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mextract_technical_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatent_application\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclaims\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mabstract_keywords\u001b[39m\u001b[33m'\u001b[39m: extract_technical_keywords(patent_application[\u001b[33m'\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     20\u001b[39m }\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Matching techniques\u001b[39;00m\n\u001b[32m     23\u001b[39m matching_scores = [\n\u001b[32m     24\u001b[39m     domain_similarity_score(examiner_expertise, application_tech_profile),\n\u001b[32m     25\u001b[39m     art_unit_alignment_score(examiner_expertise, application_tech_profile),\n\u001b[32m     26\u001b[39m     keyword_matching_score(examiner_expertise, application_tech_profile),\n\u001b[32m     27\u001b[39m     educational_background_relevance(examiner_expertise, application_tech_profile)\n\u001b[32m     28\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mextract_technical_keywords\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03mExtract technical keywords from text\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Advanced NLP-based keyword extraction\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m    120\u001b[39m nlp = spacy.load(\u001b[33m'\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m doc = nlp(text)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "def calculate_technical_specialization_match(examiner_profile, patent_application):\n",
    "    \"\"\"\n",
    "    Calculate the alignment between examiner's technical expertise \n",
    "    and the patent application's technical domain\n",
    "    \"\"\"\n",
    "    # Examiner's technical background\n",
    "    examiner_expertise = {\n",
    "        'primary_art_unit': examiner_profile['art_unit'],\n",
    "        'technical_domains': examiner_profile['technical_domains'],\n",
    "        'education_background': examiner_profile['education'],\n",
    "        'publication_areas': examiner_profile['research_publications']\n",
    "    }\n",
    "    \n",
    "    # Patent application technical characteristics\n",
    "    application_tech_profile = {\n",
    "        'primary_domain': patent_application['technical_domain'],\n",
    "        'secondary_domains': patent_application['related_domains'],\n",
    "        'claim_keywords': extract_technical_keywords(patent_application['claims']),\n",
    "        'abstract_keywords': extract_technical_keywords(patent_application['abstract'])\n",
    "    }\n",
    "    \n",
    "    # Matching techniques\n",
    "    matching_scores = [\n",
    "        domain_similarity_score(examiner_expertise, application_tech_profile),\n",
    "        art_unit_alignment_score(examiner_expertise, application_tech_profile),\n",
    "        keyword_matching_score(examiner_expertise, application_tech_profile),\n",
    "        educational_background_relevance(examiner_expertise, application_tech_profile)\n",
    "    ]\n",
    "    \n",
    "    # Weighted average of matching scores\n",
    "    weights = [0.3, 0.25, 0.25, 0.2]\n",
    "    technical_specialization_match = np.dot(matching_scores, weights)\n",
    "    \n",
    "    return technical_specialization_match\n",
    "\n",
    "def domain_similarity_score(examiner_expertise, application_tech_profile):\n",
    "    \"\"\"\n",
    "    Calculate similarity between examiner's domains and application domains\n",
    "    \"\"\"\n",
    "    examiner_domains = set(examiner_expertise['technical_domains'])\n",
    "    application_domains = set([application_tech_profile['primary_domain']] + \n",
    "                               application_tech_profile['secondary_domains'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    intersection = len(examiner_domains.intersection(application_domains))\n",
    "    union = len(examiner_domains.union(application_domains))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def art_unit_alignment_score(examiner_expertise, application_tech_profile):\n",
    "    \"\"\"\n",
    "    Assess alignment between examiner's art unit and application domain\n",
    "    \"\"\"\n",
    "    art_unit_domain_mapping = {\n",
    "        '2100': ['computer_technology', 'artificial_intelligence'],\n",
    "        '1600': ['biotechnology', 'molecular_biology'],\n",
    "        '3600': ['electrical_engineering', 'telecommunications']\n",
    "        # Add more mappings\n",
    "    }\n",
    "    \n",
    "    examiner_art_unit = examiner_expertise['primary_art_unit']\n",
    "    primary_domain = application_tech_profile['primary_domain']\n",
    "    \n",
    "    # Check if domain matches art unit domains\n",
    "    matching_domains = art_unit_domain_mapping.get(examiner_art_unit, [])\n",
    "    \n",
    "    return 1.0 if primary_domain in matching_domains else 0.0\n",
    "\n",
    "def keyword_matching_score(examiner_expertise, application_tech_profile):\n",
    "    \"\"\"\n",
    "    Match technical keywords from publications and application\n",
    "    \"\"\"\n",
    "    examiner_keywords = set(\n",
    "        keyword.lower() \n",
    "        for publication in examiner_expertise['publication_areas']\n",
    "        for keyword in extract_technical_keywords(publication)\n",
    "    )\n",
    "    \n",
    "    application_keywords = set(\n",
    "        keyword.lower()\n",
    "        for keyword_source in [\n",
    "            application_tech_profile['claim_keywords'],\n",
    "            application_tech_profile['abstract_keywords']\n",
    "        ]\n",
    "        for keyword in keyword_source\n",
    "    )\n",
    "    \n",
    "    # Calculate keyword overlap\n",
    "    intersection = len(examiner_keywords.intersection(application_keywords))\n",
    "    union = len(examiner_keywords.union(application_keywords))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def educational_background_relevance(examiner_expertise, application_tech_profile):\n",
    "    \"\"\"\n",
    "    Assess relevance of examiner's educational background\n",
    "    \"\"\"\n",
    "    education_domain_mapping = {\n",
    "        'Computer Science': ['computer_technology', 'artificial_intelligence'],\n",
    "        'Electrical Engineering': ['electronics', 'telecommunications'],\n",
    "        'Biotechnology': ['molecular_biology', 'genetic_engineering'],\n",
    "        # Add more mappings\n",
    "    }\n",
    "    \n",
    "    examiner_education = examiner_expertise['education_background']\n",
    "    primary_domain = application_tech_profile['primary_domain']\n",
    "    \n",
    "    # Check educational background domain alignment\n",
    "    matching_domains = education_domain_mapping.get(examiner_education, [])\n",
    "    \n",
    "    return 1.0 if primary_domain in matching_domains else 0.5\n",
    "\n",
    "def extract_technical_keywords(text):\n",
    "    \"\"\"\n",
    "    Extract technical keywords from text\n",
    "    \"\"\"\n",
    "    # Advanced NLP-based keyword extraction\n",
    "    import spacy\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract nouns and technical terms\n",
    "    keywords = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.pos_ in ['NOUN', 'PROPN'] and \n",
    "           len(token.lemma_) > 2\n",
    "    ]\n",
    "    \n",
    "    return list(set(keywords))\n",
    "\n",
    "# Example usage\n",
    "examiner_profile = {\n",
    "    'art_unit': '2100',\n",
    "    'technical_domains': ['computer_technology', 'machine_learning'],\n",
    "    'education': 'Computer Science',\n",
    "    'research_publications': ['Machine Learning Techniques', 'AI Applications']\n",
    "}\n",
    "\n",
    "patent_application = {\n",
    "    'technical_domain': 'artificial_intelligence',\n",
    "    'related_domains': ['machine_learning', 'data_science'],\n",
    "    'claims': 'A method for machine learning...',\n",
    "    'abstract': 'The invention relates to artificial intelligence techniques...'\n",
    "}\n",
    "\n",
    "specialization_match = calculate_technical_specialization_match(\n",
    "    examiner_profile, \n",
    "    patent_application\n",
    ")\n",
    "print(f\"Technical Specialization Match: {specialization_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
